[{"pageContent":"INFO 2950: \n\nIntro to Data Science\n\n\nLecture 20\n\n2023-11-06\n\n1","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":1,"lines":{"from":1,"to":10}}}},{"pageContent":"Agenda\n\n1.\nBayes Review\n\n2.\nText Analysis\n\n3.\nLog probability\n\n4.\nNaive Bayes Classifier\n\n\n\n2","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":2,"lines":{"from":1,"to":17}}}},{"pageContent":"What is Bayes' rule for?\n\n●\nLots of teaching examples involve drawing “balls from \nurns” because it’s an easy way to explain the concept\n\n\n●\nBut, there are lots of places you might use Bayes’ rule in \ndata science jobs, e.g.: estimating probabilities about \nspam filters detecting spam emails, given they are spam \n(or not spam)\n\n\n●\nThis is why lots of data science interviews test for things \nlike understanding Bayes’ rule!\n\n3","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":3,"lines":{"from":1,"to":19}}}},{"pageContent":"Balls in Urns!\n\n4\n\nhttps://pubs.aeaweb.org/doi/pdfplus/10.1257/jep.10.2.179","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":4,"lines":{"from":1,"to":5}}}},{"pageContent":"We secretly throw a die!\n\n5\n\nhttps://pubs.aeaweb.org/doi/pdfplus/10.1257/jep.10.2.179\n \n\n琀\n琀\n\n琀\n琀\n\n琀\n琀\n\n琀\n琀\n\n琀\n琀\n\n琀\n琀","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":5,"lines":{"from":1,"to":24}}}},{"pageContent":"We won’t show you the die roll\n\n●\nBut, we draw a single element from the “urn” \nthat was decided based on the die roll\n\n\n●\nWhat is the probability that what we drew \ncame from Urn A?\n\n6\n\nhttps://pubs.aeaweb.org/doi/pdfplus/10.1257/jep.10.2.179","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":6,"lines":{"from":1,"to":14}}}},{"pageContent":"How did you think about the \nprobability?\n\n●\nCalculation using Bayes' rule?\n\n\n\n●\nIntuition without Bayes’ rule?\n\n\n\n7","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":7,"lines":{"from":1,"to":14}}}},{"pageContent":"Interview question alert!\n\nWhat is the probability that you roll two fair dice \nand the sum of their faces is 7?\n\n\n8\n\n琀\n琀","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":8,"lines":{"from":1,"to":10}}}},{"pageContent":"Interview question alert!\n\nBrute force method:\n\n●\n[1,6],[2,5],[3,4],[4,3],[5,2],[6,1] \n→\n 6 ways to roll a 7\n\n●\n6*6 = 36 total ways to roll two dice\n\n●\n6/36 = \n⅙\n probability of rolling a 7\n\nNon-brute-force method:\n\n\n\n9","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":9,"lines":{"from":1,"to":22}}}},{"pageContent":"If the first draw was a ghost\n\n●\nCalculation using Bayes’ rule\n\n○\nP(A|ghost) = P(ghost, A) / P(ghost) \n\n\n\n\n\n10","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":10,"lines":{"from":1,"to":13}}}},{"pageContent":"If the first draw was a ghost\n\n●\nCalculation using Bayes’ rule\n\n○\nP(A|ghost) = P(ghost, A) / P(ghost) \n\n\n\n\n\n11\n\nConditional\n\nJoint / Marginal","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":11,"lines":{"from":1,"to":17}}}},{"pageContent":"If the first draw was a ghost\n\n●\nCalculation using Bayes’ rule\n\n○\nP(A|ghost) = \nP(ghost, A)\n / P(ghost) \n\n○\nP(ghost, A)\n = joint probability \n\n\n\n      = ½ * \n⅓\n = 1 / 6\n\n\n\n\n\n12","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":12,"lines":{"from":1,"to":25}}}},{"pageContent":"If the first draw was a ghost\n\n●\nCalculation using Bayes’ rule\n\n○\nP(A|ghost) = \nP(ghost, A)\n / P(ghost) \n\n○\nP(ghost, A)\n = joint probability \n\n\n\n      = ½ * \n⅓\n = 1 / 6\n\n\n\n\n\n13\n\nProb of rolling \n1, 2, 3\n\nWithin Urn A, \ngetting a ghost","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":13,"lines":{"from":1,"to":31}}}},{"pageContent":"If the first draw was a ghost\n\n●\nCalculation using Bayes’ rule\n\n○\nP(A|ghost) = P(ghost, A) / \nP(ghost)\n \n\n○\nP(ghost, A) = joint probability = 1 / 6\n\n○\nP(ghost)\n = 1/2\n\n\n\n14","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":14,"lines":{"from":1,"to":20}}}},{"pageContent":"If the first draw was a ghost\n\n●\nCalculation using Bayes’ rule\n\n○\nP(A|ghost) = P(ghost, A) / \nP(ghost)\n \n\n○\nP(ghost, A) = joint probability = 1 / 6\n\n○\nP(ghost)\n = 1/2\n\n\n\n15\n\nAcross the urns, there are 6 \nitems to draw, and 3 of \nthem are ghosts","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":15,"lines":{"from":1,"to":24}}}},{"pageContent":"If the first draw was a ghost\n\n●\nCalculation using Bayes’ rule\n\n○\nP(A|ghost)\n = P(ghost, A) / P(ghost) \n\n○\nP(ghost, A) = joint probability = 1 / 6\n\n○\nP(ghost) = 1/2\n\n\n○\nP(A|ghost)\n = \n⅙\n / ½ = \n⅓\n \n \n \n \n\n\n\n\n16","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":16,"lines":{"from":1,"to":31}}}},{"pageContent":"If the first draw was a ghost\n\n●\nCalculation using Bayes’ rule\n\n○\nP(A|ghost) = P(ghost, A) / P(ghost) \n\n○\nP(A|ghost) = \n⅙\n / ½ = \n⅓\n \n \n  \n\n\n●\nIntuition without Bayes’ rule\n\n○\n?\n\n\n17","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":17,"lines":{"from":1,"to":26}}}},{"pageContent":"If the first draw was a ghost\n\n●\nCalculation using Bayes’ rule\n\n○\nP(A|ghost) = P(ghost, A) / P(ghost) \n\n○\nP(A|ghost) = \n⅙\n / ½ = \n⅓\n \n \n  \n\n\n●\nIntuition without Bayes’ rule\n\n○\nAll 6 items are equally likely to be drawn before the \ndie is thrown!  (This is called our \nprior\n)\n\n○\n⅓\n of the ghosts overall are in Urn A \n\n\n18","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":18,"lines":{"from":1,"to":33}}}},{"pageContent":"If the first draw was a \npumpkin\n\n●\nCalculation using Bayes’ rule\n\n○\nP(A|pumpkin) = P(pumpkin, A) / P(pumpkin) \n\n○\nP(pumpkin, A)\n = joint probability = \n?\n\n○\nP(pumpkin) \n= \n?\n \n \n\n\n\n19\n\n琀\n琀","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":19,"lines":{"from":1,"to":27}}}},{"pageContent":"If the first draw was a pumpkin\n\n●\nCalculation using Bayes’ rule\n\n○\nP(A|pumpkin) = P(pumpkin, A) / P(pumpkin) \n\n○\nP(pumpkin, A) = 2/6 = \n1/3\n\n○\nP(pumpkin) = \n1/2 \n\n\n\n\n20","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":20,"lines":{"from":1,"to":20}}}},{"pageContent":"If the first draw was a pumpkin\n\n●\nCalculation using Bayes’ rule\n\n○\nP(A|pumpkin) = P(pumpkin, A) / P(pumpkin) \n\n○\nP(pumpkin, A) = 2/6 = \n1/3\n\n○\nP(pumpkin) = \n1/2 \n\n\n○\nJoint probability: Pr(rolling a 1,2,3) * Pr(drawing a \npumpkin from urn A) = ½ * \n⅔\n = 2/6 = \n⅓\n\n○\nMarginal: Pr(pumpkin) = 3 pumpkins / 6 items across \nboth urns = \n½\n \n\n\n21","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":21,"lines":{"from":1,"to":32}}}},{"pageContent":"If the first draw was a pumpkin\n\n●\nCalculation using Bayes’ rule\n\n○\nP(A|pumpkin) = P(pumpkin, A) / P(pumpkin) \n\n○\nP(pumpkin, A) = 2/6 = \n1/3\n\n○\nP(pumpkin) = \n1/2 \n\n\n○\nP(A|pumpkin) = \n⅓\n / ½ = \n⅔\n \n  \n\n\n\n22","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":22,"lines":{"from":1,"to":28}}}},{"pageContent":"If the first draw was a pumpkin\n\n●\nCalculation using Bayes’ rule\n\n○\nP(A|pumpkin) = P(pumpkin, A) / P(pumpkin) \n\n○\nP(A|pumpkin) = \n⅓\n  / ½ = \n⅔\n \n  \n\n\n●\nIntuition without Bayes’ rule\n\n○\n? \n\n23","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":23,"lines":{"from":1,"to":24}}}},{"pageContent":"If the first draw was a pumpkin\n\n●\nCalculation using Bayes’ rule\n\n○\nP(A|pumpkin) = P(pumpkin, A) / P(pumpkin) \n\n○\nP(A|pumpkin) = \n⅓\n  / ½ = \n⅔\n \n  \n\n\n●\nIntuition without Bayes’ rule\n\n○\nAll 6 items are equally likely to be drawn before the \ndie is thrown!  (This is called our \nprior\n)\n\n○\n⅔\n of the pumpkins are in Urn A \n\n24","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":24,"lines":{"from":1,"to":31}}}},{"pageContent":"Let’s assume our first draw was a \npumpkin\n\n25\n\n●\nNow we replace the pumpkin in the same \nurn we drew it out of (i.e., we drew \nwith \nreplacement\n)\n\n\n●\nAnd now, we take a second draw from \nthe \nsame urn\n as before","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":25,"lines":{"from":1,"to":18}}}},{"pageContent":"Let’s assume our first draw was a \npumpkin\n\n26\n\n●\nNow we replace the pumpkin in the same \nurn we drew it out of (i.e., we drew \nwith \nreplacement\n)\n\n\n●\nAnd now, we take a \nsecond\n draw from \nthe \nsame urn\n as before \n\n\n●\nQuestion: what is the probability that \nwhat we drew came from Urn A?\n\nhttps://pubs.aeaweb.org/doi/pdfplus/10.1257/jep.10.2.179","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":26,"lines":{"from":1,"to":27}}}},{"pageContent":"How did you think about the \nprobability?\n\n●\nCalculation using Bayes’ rule?\n\n\n\n●\nIntuition without Bayes’ rule?\n\n\n\n\n27","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":27,"lines":{"from":1,"to":15}}}},{"pageContent":"How did you think about the \nprobability?\n\n●\nCalculation using Bayes’ rule?\n\n\n\n●\nIntuition without Bayes’ rule?\n\n\n\n\n28","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":28,"lines":{"from":1,"to":15}}}},{"pageContent":"Bayes’ rule\n\n29\n\nhttps://pubs.aeaweb.org/doi/pdfplus/10.1257/jep.10.2.179","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":29,"lines":{"from":1,"to":5}}}},{"pageContent":"Bayes’ rule\n\n30\n\n●\nFirst draw from urns was a pumpkin. It is \nreturned to original urn \nwith replacement\n\n●\nAnd now, we take a second draw from the \nsame urn\n, and get a pumpkin\n\n●\nWhat is the \nprobability we drew from \n\nUrn A\n?\n\n\n\nhttps://pubs.aeaweb.org/doi/pdfplus/10.1257/jep.10.2.179","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":30,"lines":{"from":1,"to":24}}}},{"pageContent":"Bayes’ rule\n\n31\n\n●\nFirst draw from urns was a pumpkin. It is \nreturned to original urn \nwith replacement\n\n●\nAnd now, we take a second draw from the \nsame urn\n, and get a pumpkin\n\n●\nWhat is the \nprobability we drew from \n\nUrn A\n?\n\n\n●\nHow do we define A and B if we use \nBayes’ rule?\n\nhttps://pubs.aeaweb.org/doi/pdfplus/10.1257/jep.10.2.179\n \n\n琀\n琀","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":31,"lines":{"from":1,"to":31}}}},{"pageContent":"Bayes’ rule\n\n32\n\nhttps://pubs.aeaweb.org/doi/pdfplus/10.1257/jep.10.2.179\n \n\nA = drawing from urn A\n\nB = 1st draw \n\n and 2nd draw \n","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":32,"lines":{"from":1,"to":13}}}},{"pageContent":"Bayes’ rule: \nsolve\n!\n\n33\n\nhttps://pubs.aeaweb.org/doi/pdfplus/10.1257/jep.10.2.179\n \n\nA = drawing from urn A\n\nB = 1st draw \n\n and 2nd draw \n","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":33,"lines":{"from":1,"to":15}}}},{"pageContent":"Bayes’ rule\n\n34\n\nhttps://pubs.aeaweb.org/doi/pdfplus/10.1257/jep.10.2.179\n \n\nA = drawing from urn A\n\nB = 1st draw \n\n and 2nd draw \n\n\n P(A,B) = \n⅔\n * \n⅔\n =","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":34,"lines":{"from":1,"to":19}}}},{"pageContent":"Bayes’ rule\n\n35\n\nhttps://pubs.aeaweb.org/doi/pdfplus/10.1257/jep.10.2.179\n \n\nA = drawing from urn A\n\nB = 1st draw \n\n and 2nd draw \n\n\n P(A,B) = \n⅔\n * \n⅔\n =\n\n\nP(1st draw pumpkin, 2nd draw pumpkin, \nfrom Urn A) = \n⅔\n * \n⅔\n = 4/9","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":35,"lines":{"from":1,"to":27}}}},{"pageContent":"Bayes’ rule\n\n36\n\nhttps://pubs.aeaweb.org/doi/pdfplus/10.1257/jep.10.2.179\n \n\nA = drawing from urn A\n\nB = 1st draw \n\n and 2nd draw \n\n\n\n= \n⅔\n * \n⅔\n + \n⅓\n * \n⅓","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":36,"lines":{"from":1,"to":23}}}},{"pageContent":"Bayes’ rule\n\n37\n\nhttps://pubs.aeaweb.org/doi/pdfplus/10.1257/jep.10.2.179\n \n\nA = drawing from urn A\n\nB = 1st draw \n\n and 2nd draw \n\n\n\n= \n⅔\n * \n⅔\n \n+ \n⅓\n * \n⅓\n \n\n= P(B,A)\n+P(B,not-A)\n\n●\nP(1st draw pumpkin, 2nd draw \npumpkin, Urn A) = \n⅔\n * \n⅔\n = 4/9","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":37,"lines":{"from":1,"to":36}}}},{"pageContent":"Bayes’ rule\n\n38\n\nhttps://pubs.aeaweb.org/doi/pdfplus/10.1257/jep.10.2.179\n \n\nA = drawing from urn A\n\nB = 1st draw \n\n and 2nd draw \n\n\n\n= \n⅔\n * \n⅔\n +\n \n⅓\n * \n⅓\n \n\n= P(B,A)+\nP(B,not-A)\n\n●\nP(1st draw pumpkin, 2nd draw \npumpkin, Urn A) = \n⅔\n * \n⅔\n = 4/9\n\n●\nP(1st draw pumpkin, 2nd draw \npumpkin, Urn B) = \n⅓\n * \n⅓\n  = 1/9","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":38,"lines":{"from":1,"to":44}}}},{"pageContent":"Bayes’ rule\n\n39\n\nhttps://pubs.aeaweb.org/doi/pdfplus/10.1257/jep.10.2.179\n \n\nA = drawing from urn A\n\nB = 1st draw \n\n and 2nd draw \n\n\n\n= \n⅔\n * \n⅔\n + \n⅓\n * \n⅓\n \n\n= P(B,A)+P(B,not-A)\n\n●\nP(1st draw pumpkin, 2nd draw \npumpkin, Urn A) = \n⅔\n * \n⅔\n = 4/9\n\n●\nP(1st draw pumpkin, 2nd draw \npumpkin, Urn B) = \n⅓\n * \n⅓\n  = 1/9 \n \n\n●\nP(1st draw pumpkin, 2nd draw \npumpkin) = P(1st draw pumpkin, 2nd \ndraw ghost, urn A) + P(1st draw \npumpkin, 2nd draw ghost, urn B) = 5/9","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":39,"lines":{"from":1,"to":49}}}},{"pageContent":"Bayes’ rule\n\n40\n\nhttps://pubs.aeaweb.org/doi/pdfplus/10.1257/jep.10.2.179\n \n\nA = drawing from urn A\n\nB = 1st draw \n\n and 2nd draw \n\n\n\n= \n⅔\n * \n⅔\n + \n⅓\n * \n⅓\n \n\n P(A,B) = \n⅔\n * \n⅔\n =\n\n\n\n= (4/9) / (5/9) \n\n= 4/5","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":40,"lines":{"from":1,"to":36}}}},{"pageContent":"How did you think about the \nprobability?\n\n●\nCalculation using Bayes’ rule?\n\n\n\n●\nIntuition without Bayes’ rule?\n\n\n\n\n41","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":41,"lines":{"from":1,"to":15}}}},{"pageContent":"Can we express the “posterior \nbelief” after the 1st roll?\n\n42\n\nhttps://pubs.aeaweb.org/doi/pdfplus/10.1257/jep.10.2.179\n \n\nMy beliefs before \nthe 2nd draw are \nthat the probability \nthat Urn A is used is \n⅔\n (i.e. it’s twice as \nlikely that Urn A is \nused as Urn B).","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":42,"lines":{"from":1,"to":16}}}},{"pageContent":"Can we express the “posterior \nbelief” after the 1st roll?\n\n43\n\nhttps://pubs.aeaweb.org/doi/pdfplus/10.1257/jep.10.2.179\n \n\nMy beliefs before \nthe 2nd draw are \nthat the probability \nthat Urn A is used is \n⅔\n (i.e. it’s twice as \nlikely that Urn A is \nused as Urn B). \n \n\nThis figure is no longer the physical \nrepresentation of marbles but your \nbelief of the distribution of items","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":43,"lines":{"from":1,"to":21}}}},{"pageContent":"2nd draw a pumpkin: \n⅘\n prob from A\n\n44\n\nhttps://pubs.aeaweb.org/doi/pdfplus/10.1257/jep.10.2.179","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":44,"lines":{"from":1,"to":7}}}},{"pageContent":"Practice problem for home\n\n●\nFirst draw is pumpkin, second draw (with \nreplacement) is ghost. Now, what is the probability \nthat we drew from Urn A?\n\n\n45","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":45,"lines":{"from":1,"to":9}}}},{"pageContent":"2nd draw a ghost: ½ prob from A\n\n46\n\nhttps://pubs.aeaweb.org/doi/pdfplus/10.1257/jep.10.2.179","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":46,"lines":{"from":1,"to":5}}}},{"pageContent":"If the second draw was a ghost\n\n●\nP(A|1st draw pumpkin, 2nd draw ghost) = P(1st draw pumpkin, 2nd draw \nghost, A) / P(1st draw pumpkin, 2nd draw ghost) \n\n●\nP(1st draw pumpkin, 2nd draw ghost, A) = \n⅔\n * \n⅓\n  = 2/9 \n \n\n●\nP(1st draw pumpkin, 2nd draw ghost, B) = \n⅓\n * \n⅔\n  = 2/9 \n\n●\nP(1st draw pumpkin, 2nd draw ghost) = P(1st draw pumpkin, 2nd draw ghost, \nA) + P(1st draw pumpkin, 2nd draw ghost, B) = 4/9 \n\n●\nP(A|1st draw pumpkin, 2nd draw ghost) = (2/9) / (4/9) = \n1/2\n\n\n47\n\n(Marginalizing!)\n\n(Bayes’ Rule)","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":47,"lines":{"from":1,"to":35}}}},{"pageContent":"1 min break & attendance\n\n48\n\ntinyurl.com/yhajjfvk","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":48,"lines":{"from":1,"to":5}}}},{"pageContent":"Now, let’s talk about text!\n\n(we promise this will all connect back to Bayes!)\n\n49","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":49,"lines":{"from":1,"to":5}}}},{"pageContent":"Goal: classify a sentence as \npositive or negative\n\nA) \n\"I've long been searching for the best camarones diabla and \nhave found that gem here in their camarones endiablados \noffering.\"\n\nB)\n \"We walked in and nobody greeted us at the entrance and \nwe decided to walk over to one of the open tables.\"\n\nWhat do you guess (+ or -)?\n\n50\n\n琀\n琀","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":50,"lines":{"from":1,"to":18}}}},{"pageContent":"Goal: classify a sentence as \npositive or negative\n\nA) \n\"I've long been searching for the best camarones diabla and \nhave found that gem here in their camarones endiablados \noffering.\"  \n★★★★★\n\nB)\n \"We walked in and nobody greeted us at the entrance and \nwe decided to walk over to one of the open tables.\" \n★\n\n\n51\n\n+\n\n-","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":51,"lines":{"from":1,"to":20}}}},{"pageContent":"52\n\nhttps://visualist.in/sentiment-analysis-with-textblob/","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":52,"lines":{"from":1,"to":3}}}},{"pageContent":"Apply Bayes' rule\n\nP(\n★★★★★\n | \"great food, but service stank\") \n\n\n\n53","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":53,"lines":{"from":1,"to":9}}}},{"pageContent":"Apply Bayes' rule\n\nP(\n★★★★★\n | \"great food, but service stank\") =\n\n\nP(\"great food, but service stank\" | \n★★★★★\n) P(\n★★★★★\n)\n\n\n\n\n            P(\"great food, but service stank\")\n\n54","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":54,"lines":{"from":1,"to":19}}}},{"pageContent":"Apply Bayes' rule\n\nP(\n★★★★★\n | \"great food, but service stank\") =\n\n\nP(\"great food, but service stank\" | \n★★★★★\n) P(\n★★★★★\n)\n\n\n\n\n            P(\"great food, but service stank\")\n\n55","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":55,"lines":{"from":1,"to":19}}}},{"pageContent":"56\n\nhttps://en.wikipedia.org/wiki/Colorless_green_ideas_sleep_furiously","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":56,"lines":{"from":1,"to":3}}}},{"pageContent":"57\n\nBut it must be recognized \nthat the notion of \n\"probability of a sentence\" \nis an entirely useless one, \nunder any known \ninterpretation of this term.\n\nhttps://en.wikipedia.org/wiki/Noam_Chomsky","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":57,"lines":{"from":1,"to":10}}}},{"pageContent":"58\n\nhttps://rss.onlinelibrary.wiley.com/doi/full/10.1111/j.1740-9713.2012.00590.x","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":58,"lines":{"from":1,"to":3}}}},{"pageContent":"59\n\nClearly, it is inaccurate to \nsay that statistical models \n(and probabilistic models) \nhave achieved limited \nsuccess; rather they have \nachieved a dominant \n(although not exclusive) \nposition.\n\nhttps://en.wikipedia.org/wiki/Peter_Norvig","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":59,"lines":{"from":1,"to":12}}}},{"pageContent":"Assigning probability to a \nsentence\n\nP(\"great food, but service stank\" | \n★★★★★\n)\n\n60","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":60,"lines":{"from":1,"to":8}}}},{"pageContent":"Assigning probability to a \nsentence\n\nP(\"great\", \"food\", \"but\", \"service\", \"stank\" | \n★★★★★\n)\n\n61","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":61,"lines":{"from":1,"to":8}}}},{"pageContent":"Split a string into \ntokens\n\n\"the food was great but the service was bad\"\n\n\n62","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":62,"lines":{"from":1,"to":7}}}},{"pageContent":"Split a string into \ntokens\n\n\"the food was great but the service was bad\"\n\n[\"the\", \"food\", \"was\", \"great\", \"but\", \"the\", \"service\", \"was\", \"bad\"]\n\n\nWe tokenize the string into 9 word \ntokens\n\n63","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":63,"lines":{"from":1,"to":12}}}},{"pageContent":"Split a string into \ntokens\n\n\"the food was great but the service was bad\"\n\n[\"\nthe\n\", \"food\", \"\nwas\n\", \"great\", \"but\", \"\nthe\n\", \"service\", \"\nwas\n\", \"bad\"]\n\n{'the': \n2\n, 'was': \n2\n, 'food': 1, 'great': 1, 'but': 1, 'service': 1, 'bad': 1}\n\n\nThe string contains 7 distinct word \ntypes\n\n64","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":64,"lines":{"from":1,"to":26}}}},{"pageContent":"Assigning probability to a \nsentence\n\nP(\"great food, but service stank\" | \n★★★★★\n) = \n\n\nP(\"great\" | \n★★★★★\n)  x\n\n\nP(\"food\" | \n★★★★★\n)  x\n\nP(\"but\" | \n★★★★★\n)  x\n\nP(\"service\" | \n★★★★★\n)  x\n\nP(\"stank\" | \n★★★★★\n) \n\n65","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":65,"lines":{"from":1,"to":30}}}},{"pageContent":"Assigning probability to a \nsentence\n\nP(\"great food, but service stank\" | \n★★★★★\n) = \n\n\nP(\"great\" | \n★★★★★\n)  x\n\n\nP(\"food\" | \n★★★★★\n)  x\n\nP(\"but\" | \n★★★★★\n)  x\n\nP(\"service\" | \n★★★★★\n)  x\n\nP(\"stank\" | \n★★★★★\n) \n\n66\n\nApproximate \nthe probability \nof a sentence \nby multiplying \nthe probability \nof each word","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":66,"lines":{"from":1,"to":37}}}},{"pageContent":"Assigning probability to a \nsentence\n\nP(\"great food, but service stank\" | \n★★★★★\n) = \n\n\nP(\"great\" | \n★★★★★\n)  x\n\n\nP(\"food\" | \n★★★★★\n)  x\n\nP(\"but\" | \n★★★★★\n)  x\n\nP(\"service\" | \n★★★★★\n)  x\n\nP(\"stank\" | \n★★★★★\n) \n\n67\n\nNaïve\n \nassumption\n\n\nor \n\n\n\"\nbag of \nwords\n\" \nassumption","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":67,"lines":{"from":1,"to":44}}}},{"pageContent":"Bag of words: order doesn't matter\n\nP(\"great food, but service stank\" | \n★★★★★\n) = \n\n\nP(\"great service, but food stank\" | \n★★★★★\n) \n\n68","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":68,"lines":{"from":1,"to":12}}}},{"pageContent":"P(\"great food, but service stank\" | \n★★★★★\n) = \n\n\nP(\"great service, but food stank\" | \n★★★★★\n) = \n\n\nP(\"stank service food but great\" | \n★★★★★\n)\n\n69\n\nBag of words: order doesn't matter","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":69,"lines":{"from":1,"to":17}}}},{"pageContent":"P(\"great food, but service stank\" | \n★★★★★\n) = \n\n\nP(\"great service, but food stank\" | \n★★★★★\n) = \n\n\nP(\"stank service food but great\" | \n★★★★★\n)\n\n70\n\nhttps://imgflip.com/memegenerator/Star-Wars-Yod\na\n \n\nBag of words: order doesn't matter","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":70,"lines":{"from":1,"to":21}}}},{"pageContent":"Which source are we sampling?\n\n71\n\nhttps://www.walmart.com/ip/M-M-s-Milk-Chocolate-Candy-Full-Siz\ne-1-69-oz-Bag/38253480\n \n\nhttps://www.amazon.com/Tootsie-Roll-Assorted-Pops-100/dp/B003TRYMHS","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":71,"lines":{"from":1,"to":9}}}},{"pageContent":"fantastic best perfect \nincredible gem BEST perfect \ndelicious LOVE\n\nrude  worst horrible rude \nbad dirty disgusting waited \nworst worse\n\nWhich source are we sampling?\n\n72","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":72,"lines":{"from":1,"to":11}}}},{"pageContent":"Multiplying Probabilities\n\n●\nIf P(E) is the probability of event E occurring, and \nP(F) is the probability of event F occurring:\n\n\n●\nE and F are independent events\n\n\n●\nWhat is the probability that both E and F occur?\n\n73\n\n琀\n琀","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":73,"lines":{"from":1,"to":18}}}},{"pageContent":"Multiplying Probabilities\n\n●\nIf P(E) is the probability of event E occurring, and \nP(F) is the probability of event F occurring:\n\n\n●\nE and F are independent events\n\n\n●\nWhat is the probability that both E and F occur?\n\n○\nP(E and F) = P(E,F) = \nP(E) * P(F)\n\n74","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":74,"lines":{"from":1,"to":19}}}},{"pageContent":"Probability\n\n●\nIf P(E) is the probability of event E occurring, \nthen we know: \n\n75\n\nhttps://chrispiech.github.io/probabilityForComputerScientists/en/part1/log_probabilities/\n \n\n琀\n琀","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":75,"lines":{"from":1,"to":13}}}},{"pageContent":"Probability\n\n●\nIf P(E) is the probability of event E occurring, \nthen we know: \n\n76\n\nhttps://chrispiech.github.io/probabilityForComputerScientists/en/part1/log_probabilities/","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":76,"lines":{"from":1,"to":9}}}},{"pageContent":"Log Probability\n\n77\n\nhttps://en.wikipedia.org/wiki/Log_probability","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":77,"lines":{"from":1,"to":5}}}},{"pageContent":"Log Probability\n\n●\nIf P(E) is the probability of event E occurring, \nthen we know: \n\n78\n\nhttps://chrispiech.github.io/probabilityForComputerScientists/en/part1/log_probabilities/\n \n\n琀\n琀","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":78,"lines":{"from":1,"to":13}}}},{"pageContent":"Log Probability\n\n●\nIf P(E) is the probability of event E occurring, \nthen we know: \n\n79\n\nhttps://chrispiech.github.io/probabilityForComputerScientists/en/part1/log_probabilities/","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":79,"lines":{"from":1,"to":9}}}},{"pageContent":"Log Probability\n\n●\nIf P(E) is the probability of event E occurring, and \nP(F) is the probability of event F occurring:\n\n80\n\nhttps://chrispiech.github.io/probabilityForComputerScientists/en/part1/log_probabilities/\n \n\n琀\n琀","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":80,"lines":{"from":1,"to":13}}}},{"pageContent":"Log Probability\n\n●\nIf P(E) is the probability of event E occurring, and \nP(F) is the probability of event F occurring:\n\n81\n\nhttps://chrispiech.github.io/probabilityForComputerScientists/en/part1/log_probabilities/","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":81,"lines":{"from":1,"to":9}}}},{"pageContent":"Log Probability\n\n●\nIf we already know the values of P(E) and P(F), \nwhy would we want to take the logarithm?\n\n82\n\nhttps://chrispiech.github.io/probabilityForComputerScientists/en/part1/log_probabilities/","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":82,"lines":{"from":1,"to":9}}}},{"pageContent":"Log Probability\n\n83\n\nOne in \n\nProbability\n\nLog\n10\n\nLog\ne\n\n10\n\n0.1\n\n-1\n\n-2.3\n\n100\n\n0.01\n\n-2\n\n-4.6\n\n1,000\n\n0.001\n\n-3\n\n-6.9\n\n10,000\n\n0.0001\n\n-4\n\n-9.2\n\n100,000\n\n0.00001\n\n-5\n\n-11.5\n\nLogs are much more \nunderstandable \nwhen probabilities \nare small!","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":83,"lines":{"from":1,"to":58}}}},{"pageContent":"Log Probability\n\n●\nBefore, we’ve talked \nabout log transforms as \n“squishing” big numbers\n\n\n\n\n84\n\nhttps://en.wikipedia.org/wiki/Logarithm","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":84,"lines":{"from":1,"to":13}}}},{"pageContent":"Log Probability\n\n●\nBefore, we’ve talked \nabout log transforms as \n“squishing” big numbers\n\n\n●\nBut now we’re \nconstrained to log(P(E)) \nwhere 0 < P(E) < 1, where \nwe’re actually \nnot\n \nsquishing numbers\n\n\n85\n\nhttps://en.wikipedia.org/wiki/Logarithm","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":85,"lines":{"from":1,"to":21}}}},{"pageContent":"86","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":86,"lines":{"from":1,"to":1}}}},{"pageContent":"Log Probability\n\n●\nIs the logarithm function (base \ne\n) \n\n\n○\nA monotonic function?\n\n\n○\nIncreasing? Decreasing?\n\n87\n\n琀\n琀","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":87,"lines":{"from":1,"to":19}}}},{"pageContent":"Log Probability\n\n●\nIs the logarithm function (base \ne\n) \n\n\n○\nA monotonic function?  \nYes\n\n\n○\nIncreasing? Decreasing?  \nIncreasing\n\n88","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":88,"lines":{"from":1,"to":18}}}},{"pageContent":"Log Probability\n\n●\nWhy is it convenient that ln (log base \ne\n) is a \nmonotonic function?\n\n\n○\nThe input point at which \na*b\n is maximized is \nthe same input point at which log(a*b) is \nmaximized\n\n\n89","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":89,"lines":{"from":1,"to":18}}}},{"pageContent":"Log Probability\n\n●\nWhy is it convenient that ln (log base \ne\n) is a \nmonotonic function?\n\n\n○\nThe input point at which \na*b\n is maximized is \nthe same input point at which log(a*b) is \nmaximized\n\n■\nThis is called the \nargmax \nbecause we're \nsorting by a \nfunction\n of the point, not the \npoint itself\n\n90","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":90,"lines":{"from":1,"to":26}}}},{"pageContent":"Log Probability\n\n●\nThe key is in what we derived before: \n\n91\n\nhttps://chrispiech.github.io/probabilityForComputerScientists/en/part1/log_probabilities/","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":91,"lines":{"from":1,"to":8}}}},{"pageContent":"Log Probability\n\n●\nThe key is in what we derived before: \n\n92\n\nhttps://chrispiech.github.io/probabilityForComputerScientists/en/part1/log_probabilities/","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":92,"lines":{"from":1,"to":8}}}},{"pageContent":"Log Probability\n\n●\nThe key is in what we derived before: \n\n93\n\nhttps://chrispiech.github.io/probabilityForComputerScientists/en/part1/log_probabilities/\n \n\nWhat happens if you multiply a bunch of \ntiny numbers close to 0?","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":93,"lines":{"from":1,"to":12}}}},{"pageContent":"Log Probability\n\n●\nThe key is in what we derived before: \n\n94\n\nhttps://chrispiech.github.io/probabilityForComputerScientists/en/part1/log_probabilities/\n \n\nWhat happens if you multiply a bunch of tiny numbers \nclose to 0? \n Unless you have a really special computer, \nyou’ll get 0","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":94,"lines":{"from":1,"to":14}}}},{"pageContent":"Log Probability\n\n●\nThe key is in what we derived before: \n\n95\n\nhttps://chrispiech.github.io/probabilityForComputerScientists/en/part1/log_probabilities/\n \n\nThe smallest decimal Python can \nexpress is 2.225e-308","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":95,"lines":{"from":1,"to":12}}}},{"pageContent":"Log Probability\n\n●\nThe key is in what we derived before: \n\n96\n\nhttps://chrispiech.github.io/probabilityForComputerScientists/en/part1/log_probabilities/\n \n\nDo we run into small number \nissues if we do sums instead?","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":96,"lines":{"from":1,"to":12}}}},{"pageContent":"Log Probability\n\n●\nThe key is in what we derived before: \n\n97\n\nhttps://chrispiech.github.io/probabilityForComputerScientists/en/part1/log_probabilities/\n \n\nNo issues! log(2.225e-308) = \n-307.652, easy to store in Python","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":97,"lines":{"from":1,"to":12}}}},{"pageContent":"Log Probability\n\n>>> import numpy as np\n\n\n>>> r = np.random.random_sample((100,)) * 0.000001\n\n\n98\n\nhttps://math.stackexchange.com/questions/892832/why-we-consider-log-likelihood-instead-of-likelihood-in-gaussian-distribution\n \n\nSimulate small \nprobabilities","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":98,"lines":{"from":1,"to":15}}}},{"pageContent":"Log Probability\n\n>>> import numpy as np\n\n\n>>> r = np.random.random_sample((100,)) * 0.000001\n\n\n>>> np.product(r)\n\n\n\n99\n\nhttps://math.stackexchange.com/questions/892832/why-we-consider-log-likelihood-instead-of-likelihood-in-gaussian-distribution\n \n\nWhat will we get?","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":99,"lines":{"from":1,"to":18}}}},{"pageContent":"Log Probability\n\n>>> import numpy as np\n\n\n>>> r = np.random.random_sample((100,)) * 0.000001\n\n\n>>> np.product(r)\n\n0.0\n\n\n100\n\nhttps://math.stackexchange.com/questions/892832/why-we-consider-log-likelihood-instead-of-likelihood-in-gaussian-distribution\n \n\nNot the actual \nproduct – too \nsmall for Python!","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":100,"lines":{"from":1,"to":21}}}},{"pageContent":"Log Probability\n\n>>> import numpy as np\n\n\n>>> r = np.random.random_sample((100,)) * 0.000001\n\n\n>>> np.product(r)\n\n0.0\n\n\n>>> log_r = [np.log(x) for x in r]\n\n101\n\nhttps://math.stackexchange.com/questions/892832/why-we-consider-log-likelihood-instead-of-likelihood-in-gaussian-distribution\n \n\nLet’s try taking \nlogs now","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":101,"lines":{"from":1,"to":22}}}},{"pageContent":"Log Probability\n\n>>> import numpy as np\n\n\n>>> r = np.random.random_sample((100,)) * 0.000001\n\n\n>>> np.product(r)\n\n0.0\n\n\n>>> log_r = [np.log(x) for x in r]\n\n\n>>> np.sum(log_r)\n\n\n102\n\nhttps://math.stackexchange.com/questions/892832/why-we-consider-log-likelihood-instead-of-likelihood-in-gaussian-distribution\n \n\nDo we expect a \nweird number now?","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":102,"lines":{"from":1,"to":26}}}},{"pageContent":"Log Probability\n\n>>> import numpy as np\n\n\n>>> r = np.random.random_sample((100,)) * 0.000001\n\n\n>>> np.product(r)\n\n0.0\n\n\n>>> log_r = [np.log(x) for x in r]\n\n\n>>> np.sum(log_r)\n\n-1472.245511811776\n\n103\n\nhttps://math.stackexchange.com/questions/892832/why-we-consider-log-likelihood-instead-of-likelihood-in-gaussian-distribution\n \n\nNo Python \nissues here!","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":103,"lines":{"from":1,"to":27}}}},{"pageContent":"Log Probability Takeaways\n\n●\nIf you’re using a computer to help you calculate \nthe product of probabilities, you should nearly \nalways just use log probabilities instead\n\n\n○\nThey are harder for humans to interpret...\n\n○\nbut will allow for more accurate computation\n\n○\nand allow for finding the same maximizing \npoints due to monotonicity \n\n104","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":104,"lines":{"from":1,"to":19}}}},{"pageContent":"1 min break \n(stare at this table)\n\n105\n\nOne in \n\nProbability\n\nLog\n10\n\nLog\ne\n\n10\n\n0.1\n\n-1\n\n-2.3\n\n100\n\n0.01\n\n-2\n\n-4.6\n\n1,000\n\n0.001\n\n-3\n\n-6.9\n\n10,000\n\n0.0001\n\n-4\n\n-9.2\n\n100,000\n\n0.00001\n\n-5\n\n-11.5","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":105,"lines":{"from":1,"to":54}}}},{"pageContent":"Log prob quiz!\n\nWhat probability has log\ne\n \n-2.3\n?\n\n\nOne in ________\n\nWhat is the log\ne\n of \nOne in 100,000\n?\n\n\n________\n\n106\n\n琀\n琀","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":106,"lines":{"from":1,"to":24}}}},{"pageContent":"Log prob quiz!\n\nWhat probability has log\ne\n \n-2.3\n?\n\n\nOne in ___\n10\n____\n\nWhat is the log\ne\n of \nOne in 100,000\n?\n\n\n___\n-11.5\n_____\n\n107","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":107,"lines":{"from":1,"to":25}}}},{"pageContent":"Classifying text: sports or not?\n\n108\n\nText\n\nTag\n\n\"A great game\"\n\nSports\n\n\"The election was over\"\n\nNot sports\n\n\"Very clean match\"\n\nSports\n\n\"A clean but forgettable game\"\n\nSports\n\n\"It was a close election\"\n\nNot sports\n\nhttps://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":108,"lines":{"from":1,"to":29}}}},{"pageContent":"Classifying text: sports or not?\n\n109\n\nText\n\nTag\n\n\"A great game\"\n\nSports\n\n\"The election was over\"\n\nNot sports\n\n\"Very clean match\"\n\nSports\n\n\"A clean but forgettable game\"\n\nSports\n\n\"It was a close election\"\n\nNot sports\n\nTraining data\n\nhttps://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":109,"lines":{"from":1,"to":31}}}},{"pageContent":"Classifying text: sports or not?\n\n110\n\nText\n\nTag\n\n\"A great game\"\n\nSports\n\n\"The election was over\"\n\nNot sports\n\n\"Very clean match\"\n\nSports\n\n\"A clean but forgettable game\"\n\nSports\n\n\"It was a close election\"\n\nNot sports\n\n\"A very close game\"\n\n\nTraining data\n\nWant to \nclassify new \ndata\n\nhttps://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":110,"lines":{"from":1,"to":38}}}},{"pageContent":"Classifying text: sports or not?\n\n111\n\nText\n\nTag\n\n\"A very close game\"\n\nSports / Not Sports?\n\nWant to \nclassify new \ndata\n\nIn math: P(_____ | _______)?\n\nhttps://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/\n \n\n琀\n琀","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":111,"lines":{"from":1,"to":23}}}},{"pageContent":"Classifying text: sports or not?\n\n112\n\nText\n\nTag\n\n\"A very close game\"\n\nSports / Not Sports?\n\nWant to \nclassify new \ndata\n\nIn math: P(Sports | “A very close game”)\n\nhttps://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":112,"lines":{"from":1,"to":19}}}},{"pageContent":"Classifying text: sports or not?\n\n113\n\nText\n\nTag\n\n\"A very close game\"\n\nSports / Not Sports?\n\nWant to \nclassify new \ndata\n\nIn math: P(Sports | “A very close game”)\n\nNotice: we aren’t outputting a binary estimate of \nSports or Not Sports – we’re outputting an \nestimate of the probability that our tag is Sports\n\nhttps://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":113,"lines":{"from":1,"to":23}}}},{"pageContent":"Classifying text: sports or not?\n\n114\n\nText\n\nTag\n\n\"A very close game\"\n\nSports / Not Sports?\n\nWant to \nclassify new \ndata\n\nIn math: P(Sports | “A very close game”)\n\n“Sports” here is a binary variable: it can take either \nvalue 0 or 1.  We can, for example, write a subset \nprobability: P(Sports=1 | “A very close game”)\n\nhttps://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":114,"lines":{"from":1,"to":23}}}},{"pageContent":"Classifying text: sports or not?\n\n115\n\nText\n\nTag\n\n\"A great game\"\n\nSports\n\n\"The election was over\"\n\nNot sports\n\n\"Very clean match\"\n\nSports\n\n\"A clean but forgettable game\"\n\nSports\n\n\"It was a close election\"\n\nNot sports\n\nHow do we use \ntraining data\n to calculate \nP(Sports | “A very close \ngame”)\n?\n\nhttps://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":115,"lines":{"from":1,"to":36}}}},{"pageContent":"Classifying text: sports or not?\n\n116\n\nFirst, let’s apply Bayes’ theorem:\n\nhttps://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/\n \n\nP(Sports | “A very close game”) \n= \n\nP(“A very close game” | Sports) * P(Sports) \n \n\nP(“A very close \ngame”)","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":116,"lines":{"from":1,"to":17}}}},{"pageContent":"Classifying text: sports or not?\n\n117\n\nFirst, let’s apply Bayes’ theorem:\n\nhttps://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/\n \n\nP(Sports | “A very close game”) \n= \n\nP(“A very close game” | Sports) * P(Sports) \n \n\nP(“A very close \ngame”) \n \n\nRemember we’re being naive today... let’s \nassume all the words are \nindependent\n (Yoda)","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":117,"lines":{"from":1,"to":23}}}},{"pageContent":"Classifying text: sports or not?\n\n118\n\nAssuming words are independent:\n\nhttps://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/\n \n\nP(Sports | “A”, “very”, “close”, game”) \n= \n\nP(“A”, “very”, “close”, “game” | Sports) * \nP(Sports)\n\nP(“A”, “very”, “close”, \n“game”)","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":118,"lines":{"from":1,"to":17}}}},{"pageContent":"Classifying text: sports or not?\n\n119\n\nhttps://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/\n, \nhttps://www.kdnuggets.com/2020/06/naive-bayes-algorithm-everything.html\n \n \n\nP(Sports | “A”, “very”, “close”, game”) \n= \n\nP(“A”, “very”, “close”, “game” | Sports) * \nP(Sports)\n\nP(“A”, “very”, “close”, \n“game”)","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":119,"lines":{"from":1,"to":18}}}},{"pageContent":"Classifying text: sports or not?\n\n120\n\nhttps://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/\n, \nhttps://www.kdnuggets.com/2020/06/naive-bayes-algorithm-everything.html\n \n \n\nP(Sports | “A”, “very”, “close”, game”) \n= \n\nP(“A”, “very”, “close”, “game” | Sports) * \nP(Sports)\n\nP(“A”, “very”, “close”, \n“game”) \n \n\nWe can reduce further using the “chain rule”","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":120,"lines":{"from":1,"to":21}}}},{"pageContent":"Classifying text: sports or not?\n\n121\n\nhttps://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/\n, \nhttps://www.kdnuggets.com/2020/06/naive-bayes-algorithm-everything.html\n \n \n\nP(Sports | “A very close game”) = \n\nP(“A”|Sports)*P(“very”|Sports)*P(“close”|Sports)*P(“game”|Sports) * P(Sports)\n\nP(“A”, “very”, “close”, \n“game”)","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":121,"lines":{"from":1,"to":16}}}},{"pageContent":"Classifying text: sports or not?\n\n122\n\nhttps://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/\n, \nhttps://www.kdnuggets.com/2020/06/naive-bayes-algorithm-everything.html\n \n \n\nP(Sports | “A very close game”) = \n\nP(“A”|Sports)*P(“very”|Sports)*P(“close”|Sports)*P(“game”|Sports) * P(Sports)\n\nP(“A”, “very”, “close”, \n“game”) \n \n\nWe can reduce this slightly more!","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":122,"lines":{"from":1,"to":19}}}},{"pageContent":"Classifying text: sports or not?\n\n123\n\nhttps://www.kdnuggets.com/2020/06/naive-bayes-algorithm-everything.html","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":123,"lines":{"from":1,"to":5}}}},{"pageContent":"Classifying text: sports or not?\n\n124\n\nhttps://www.kdnuggets.com/2020/06/naive-bayes-algorithm-everything.html","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":124,"lines":{"from":1,"to":5}}}},{"pageContent":"Classifying text: sports or not?\n\n125\n\nhttps://www.kdnuggets.com/2020/06/naive-bayes-algorithm-everything.html\n \n \n\nThese are the same; big Pi means \nmultiplying (the same way big Sigma \nmeans summing)","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":125,"lines":{"from":1,"to":11}}}},{"pageContent":"Classifying text: sports or not?\n\n126\n\nhttps://www.kdnuggets.com/2020/06/naive-bayes-algorithm-everything.html\n \n \n\nalpha here means “proportional to” (e.g., instead “=” which means “equals to”)","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":126,"lines":{"from":1,"to":9}}}},{"pageContent":"Classifying text: sports or not?\n\n127\n\nhttps://www.kdnuggets.com/2020/06/naive-bayes-algorithm-everything.html\n \n \n\nThis is just the numerator of the above \nexpression!  What happened to the \ndenominator?","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":127,"lines":{"from":1,"to":11}}}},{"pageContent":"Classifying text: sports or not?\n\n128\n\nhttps://www.kdnuggets.com/2020/06/naive-bayes-algorithm-everything.html\n \n \n\nThe goal of calculating P(y|x\n1\n, ..., x\nn\n) is to find the argmax of it!","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":128,"lines":{"from":1,"to":13}}}},{"pageContent":"Classifying text: sports or not?\n\n129\n\nhttps://www.kdnuggets.com/2020/06/naive-bayes-algorithm-everything.html","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":129,"lines":{"from":1,"to":5}}}},{"pageContent":"Classifying text: sports or not?\n\n130\n\nhttps://www.kdnuggets.com/2020/06/naive-bayes-algorithm-everything.html\n \n \n\nIt turns out the denominator isn’t affected by \ny\n, so it’ll just be a \nconstant that’s irrelevant for calculating the argmax!","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":130,"lines":{"from":1,"to":12}}}},{"pageContent":"Classifying text: sports or not?\n\n131\n\nhttps://www.kdnuggets.com/2020/06/naive-bayes-algorithm-everything.html\n \n \n\nWhat does this mean in words? Let’s switch back to our example...","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":131,"lines":{"from":1,"to":9}}}},{"pageContent":"Classifying text: sports or not?\n\n132\n\nhttps://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/\n, \nhttps://www.kdnuggets.com/2020/06/naive-bayes-algorithm-everything.html\n \n \n\nP(Sports | “A very close game”) = \n\nP(“A”|Sports)*P(“very”|Sports)*P(“close”|Sports)*P(“game”|Sports) * P(Sports)\n\nP(“A”, “very”, “close”, \n“game”)","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":132,"lines":{"from":1,"to":16}}}},{"pageContent":"Classifying text: sports or not?\n\n133\n\nhttps://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/\n, \nhttps://www.kdnuggets.com/2020/06/naive-bayes-algorithm-everything.html\n \n \n\nP(Sports | “A very close game”) = \n\nP(“A”|Sports)*P(“very”|Sports)*P(“close”|Sports)*P(“game”|Sports) * P(Sports)\n\nP(“A”, “very”, “close”, \n“game”) \n \n\nWe have y = Sports, and x = “A very close game”\n\nWe want to pick the value of y (Sports = 1 or Sports = 0) such that the probability \nabove is maximized.","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":133,"lines":{"from":1,"to":22}}}},{"pageContent":"Classifying text: sports or not?\n\n134\n\nhttps://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/\n, \nhttps://www.kdnuggets.com/2020/06/naive-bayes-algorithm-everything.html\n \n \n\nP(Sports | “A very close game”) = \n\nP(“A”|Sports)*P(“very”|Sports)*P(“close”|Sports)*P(“game”|Sports) * P(Sports)\n\nP(“A”, “very”, “close”, \n“game”) \n \n\nP(“A”, “very”, “close”, “game”) is \nthe same\n for Sports and NotSports.\n\nWe want to pick the value of y (Sports = 1 or Sports = 0) such that the probability \nabove is maximized.","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":134,"lines":{"from":1,"to":24}}}},{"pageContent":"Classifying text: sports or not?\n\n135\n\nhttps://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/\n, \nhttps://www.kdnuggets.com/2020/06/naive-bayes-algorithm-everything.html\n \n \n\nP(Sports | “A very close game”) = \n\nP(“A”|Sports)*P(“very”|Sports)*P(“close”|Sports)*P(“game”|Sports) * P(Sports)\n\nP(“A”, “very”, “close”, \n“game”) \n \n\n흰\n  P(“A”|Sports)*P(“very”|Sports)*P(“close”|Sports)*P(“game”|Sports) * P(Sports)\n\nalpha-looking thing means “proportional to”","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":135,"lines":{"from":1,"to":22}}}},{"pageContent":"Classifying text: sports or not?\n\n136\n\nhttps://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/\n, \nhttps://www.kdnuggets.com/2020/06/naive-bayes-algorithm-everything.html\n \n \n\nTo calculate the probability P(Sports | “A very close \ngame”), we need the following ingredients: \n\n●\nP(“A”|Sports)\n\n●\nP(“very”|Sports)\n\n●\nP(“close”|Sports)\n\n●\nP(“game”|Sports)\n\n●\nP(Sports)\n\nHow do we get P(“word”|Sports)??","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":136,"lines":{"from":1,"to":29}}}},{"pageContent":"Generate word frequencies!\n\n137\n\nText\n\nTag\n\n\"A great game\"\n\nSports\n\n\"The election was over\"\n\nNot sports\n\n\"Very clean match\"\n\nSports\n\n\"A clean but forgettable game\"\n\nSports\n\n\"It was a close election\"\n\nNot sports\n\nhttps://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/\n \n\nWord\n\n# in Sports\n\n# in Not sports\n\n“a”\n\n\n\n“very”\n\n\n\n“close”\n\n\n\n“game”\n\n\n\nTraining Data\n\nJoint Count Data","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":137,"lines":{"from":1,"to":56}}}},{"pageContent":"Generate word frequencies!\n\n138\n\nText\n\nTag\n\n\"\nA\n great game\"\n\nSports\n\n\"The election was over\"\n\nNot sports\n\n\"Very clean match\"\n\nSports\n\n\"\nA\n clean but forgettable game\"\n\nSports\n\n\"It was \na\n close election\"\n\nNot sports\n\nhttps://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/\n \n\nWord\n\n# in Sports\n\n# in Not sports\n\n“a”\n\n2\n\n1\n\n“very”\n\n\n\n“close”\n\n\n\n“game”\n\n\n\nTraining Data\n\nJoint Count Data","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":138,"lines":{"from":1,"to":64}}}},{"pageContent":"Generate word frequencies!\n\n139\n\nText\n\nTag\n\n\"A great game\"\n\nSports\n\n\"The election was over\"\n\nNot sports\n\n\"\nVery\n clean match\"\n\nSports\n\n\"A clean but forgettable game\"\n\nSports\n\n\"It was a close election\"\n\nNot sports\n\nhttps://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/\n \n\nWord\n\n# in Sports\n\n# in Not sports\n\n“a”\n\n2\n\n1\n\n“very”\n\n1\n\n0\n\n“close”\n\n\n\n“game”\n\n\n\nTraining Data\n\nJoint Count Data","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":139,"lines":{"from":1,"to":62}}}},{"pageContent":"Generate word frequencies!\n\n140\n\nText\n\nTag\n\n\"A great game\"\n\nSports\n\n\"The election was over\"\n\nNot sports\n\n\"Very clean match\"\n\nSports\n\n\"A clean but forgettable game\"\n\nSports\n\n\"It was a close election\"\n\nNot sports\n\nhttps://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/\n \n\nWord\n\n# in Sports\n\n# in Not sports\n\n“a”\n\n2\n\n1\n\n“very”\n\n1\n\n0\n\n“close”\n\n?\n\n?\n\n“game”\n\n\n\nTraining Data\n\nJoint Count Data\n\n琀\n琀","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":140,"lines":{"from":1,"to":65}}}},{"pageContent":"Generate word frequencies!\n\n141\n\nText\n\nTag\n\n\"A great game\"\n\nSports\n\n\"The election was over\"\n\nNot sports\n\n\"Very clean match\"\n\nSports\n\n\"A clean but forgettable game\"\n\nSports\n\n\"It was a \nclose\n election\"\n\nNot sports\n\nhttps://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/\n \n\nWord\n\n# in Sports\n\n# in Not sports\n\n“a”\n\n2\n\n1\n\n“very”\n\n1\n\n0\n\n“close”\n\n0\n\n1\n\n“game”\n\n\n\nTraining Data\n\nJoint Count Data","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":141,"lines":{"from":1,"to":64}}}},{"pageContent":"Generate word frequencies!\n\n142\n\nText\n\nTag\n\n\"A great game\"\n\nSports\n\n\"The election was over\"\n\nNot sports\n\n\"Very clean match\"\n\nSports\n\n\"A clean but forgettable game\"\n\nSports\n\n\"It was a close election\"\n\nNot sports\n\nhttps://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/\n \n\nWord\n\n# in Sports\n\n# in Not sports\n\n“a”\n\n2\n\n1\n\n“very”\n\n1\n\n0\n\n“close”\n\n0\n\n1\n\n“game”\n\n2\n\n0\n\nTraining Data\n\nJoint Count Data","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":142,"lines":{"from":1,"to":64}}}},{"pageContent":"Classifying text: sports or not?\n\n143\n\nhttps://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/\n, \nhttps://www.kdnuggets.com/2020/06/naive-bayes-algorithm-everything.html\n \n \n\nFor P(Sports = 1 | “A very close game”), \nmultiply:\n\n●\nP(“A”|Sports)\n\n●\nP(“very”|Sports)\n\n●\nP(“close”|Sports)\n\n●\nP(“game”|Sports)\n\n●\nP(Sports)\n\n●\nP(“A”|Not sports)\n\n●\nP(“very”|Not sports)\n\n●\nP(“close”|Not sports)\n\n●\nP(“game”|Not sports)\n\n●\nP(Not sports)\n\nFor P(Sports = 0 | “A very close game”), \nmultiply:","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":143,"lines":{"from":1,"to":45}}}},{"pageContent":"Classifying text: sports or not?\n\n144\n\nhttps://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/\n, \nhttps://www.kdnuggets.com/2020/06/naive-bayes-algorithm-everything.html\n \n \n\nFor P(Sports = 1 | “A very close game”), \nmultiply:\n\n●\nP(“A”|Sports)\n\n●\nP(“very”|Sports)\n\n●\nP(“close”|Sports)\n\n●\nP(“game”|Sports)\n\n●\nP(Sports)\n\n●\nP(“A”|Not sports)\n\n●\nP(“very”|Not sports)\n\n●\nP(“close”|Not sports)\n\n●\nP(“game”|Not sports)\n\n●\nP(Not sports)\n\nFor P(Sports = 0 | “A very close game”), \nmultiply:\n\nGoal: compare which probability is bigger!","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":144,"lines":{"from":1,"to":47}}}},{"pageContent":"Classifying text: sports or not?\n\n145\n\nhttps://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/\n, \nhttps://www.kdnuggets.com/2020/06/naive-bayes-algorithm-everything.html\n \n \n\nFor P(Sports = 1 | “A very close game”), \nmultiply:\n\n●\nP(“A”|Sports)\n\n●\nP(“very”|Sports)\n\n●\nP(“close”|Sports)\n\n●\nP(“game”|Sports)\n\n●\nP(Sports)\n\n●\nP(“A”|Not sports)\n\n●\nP(“very”|Not sports)\n\n●\nP(“close”|Not sports)\n\n●\nP(“game”|Not sports)\n\n●\nP(Not sports)\n\nFor P(Sports = 0 | “A very close game”), \nmultiply:\n\nOur joint count table gave us these numbers","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":145,"lines":{"from":1,"to":47}}}},{"pageContent":"Classifying text: sports or not?\n\n146\n\nhttps://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/\n, \nhttps://www.kdnuggets.com/2020/06/naive-bayes-algorithm-everything.html\n \n \n\nFor P(Sports = 1 | “A very close game”), \nmultiply:\n\n●\nP(“A”|Sports)\n\n●\nP(“very”|Sports)\n\n●\nP(“close”|Sports)\n\n●\nP(“game”|Sports)\n\n●\nP(Sports)\n\n●\nP(“A”|Not sports)\n\n●\nP(“very”|Not sports)\n\n●\nP(“close”|Not sports)\n\n●\nP(“game”|Not sports)\n\n●\nP(Not sports)\n\nFor P(Sports = 0 | “A very close game”), \nmultiply:\n\nBut we still don’t have these probabilities!","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":146,"lines":{"from":1,"to":47}}}},{"pageContent":"147\n\nText\n\nTag\n\n\"A great game\"\n\nSports\n\n\"The election was over\"\n\nNot sports\n\n\"Very clean match\"\n\nSports\n\n\"A clean but forgettable game\"\n\nSports\n\n\"It was a close election\"\n\nNot sports\n\nhttps://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/\n \n\nP(Sports) = \n?\n\n\nP(Not sports) = \n?\n\n琀\n琀","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":147,"lines":{"from":1,"to":38}}}},{"pageContent":"148\n\nText\n\nTag\n\n\"A great game\"\n\nSports\n\n\"The election was over\"\n\nNot sports\n\n\"Very clean match\"\n\nSports\n\n\"A clean but forgettable game\"\n\nSports\n\n\"It was a close election\"\n\nNot sports\n\nhttps://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/\n \n\nP(Sports) = \n3/5\n\n\nP(Not sports) = 2/5","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":148,"lines":{"from":1,"to":34}}}},{"pageContent":"149\n\nText\n\nTag\n\n\"A great game\"\n\nSports\n\n\"The election was over\"\n\nNot sports\n\n\"Very clean match\"\n\nSports\n\n\"A clean but forgettable game\"\n\nSports\n\n\"It was a close election\"\n\nNot sports\n\nhttps://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/\n \n\nP(Sports) = 3/5\n\nP(Not sports) = 2/5\n\n\n# words in Sports = 11\n\n# words in Not sports = 9","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":149,"lines":{"from":1,"to":37}}}},{"pageContent":"150\n\nText\n\nTag\n\n\"A great game\"\n\nSports\n\n\"The election was over\"\n\nNot sports\n\n\"Very clean match\"\n\nSports\n\n\"A clean but forgettable game\"\n\nSports\n\n\"It was a close election\"\n\nNot sports\n\nhttps://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/\n \n\nWord\n\n# in Sports\n\n# in Not sports\n\nP(Word|Sports)\n\nP(Word|Not sports)\n\n“a”\n\n2\n\n1\n\n?\n\n?\n\n“very”\n\n1\n\n0\n\n?\n\n?\n\n“close”\n\n0\n\n1\n\n?\n\n?\n\n“game”\n\n2\n\n0\n\n?\n\n?\n\nP(Sports) = 3/5\n\nP(Not sports) = 2/5\n\n\n# words in Sports = 11\n\n# words in Not sports = 9","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":150,"lines":{"from":1,"to":87}}}},{"pageContent":"151\n\nText\n\nTag\n\n\"A great game\"\n\nSports\n\n\"The election was over\"\n\nNot sports\n\n\"Very clean match\"\n\nSports\n\n\"A clean but forgettable game\"\n\nSports\n\n\"It was a close election\"\n\nNot sports\n\nhttps://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/\n \n\nWord\n\n# in Sports\n\n# in Not sports\n\nP(Word|Sports)\n\nP(Word|Not sports)\n\n“a”\n\n2\n\n1\n\n2/11\n\n1/9\n\n“very”\n\n1\n\n0\n\n1/11\n\n0/9\n\n“close”\n\n0\n\n1\n\n0/11\n\n1/9\n\n“game”\n\n2\n\n0\n\n2/11\n\n0/9\n\nP(Sports) = 3/5\n\nP(Not sports) = 2/5\n\n\n# words in Sports = 11\n\n# words in Not sports = 9","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":151,"lines":{"from":1,"to":87}}}},{"pageContent":"152\n\nText\n\nTag\n\n\"A great game\"\n\nSports\n\n\"The election was over\"\n\nNot sports\n\n\"Very clean match\"\n\nSports\n\n\"A clean but forgettable game\"\n\nSports\n\n\"It was a close election\"\n\nNot sports\n\nhttps://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/\n \n\nWord\n\n# in Sports\n\n# in Not sports\n\nP(Word|Sports)\n\nP(Word|Not sports)\n\n“a”\n\n2\n\n1\n\n2/11\n\n1/9\n\n“very”\n\n1\n\n0\n\n1/11\n\n0/9\n\n“close”\n\n0\n\n1\n\n0/11\n\n1/9\n\n“game”\n\n2\n\n0\n\n2/11\n\n0/9\n\nWe want to multiply...\n\n●\nP(“A”|Sports) = 2/11\n\n●\nP(“very”|Sports) = 1/11 \n\n●\nP(“close”|Sports) =\n 0/11\n\n●\nP(“game”|Sports) = 2/11\n\n●\nP(Sports) = 3/5","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":152,"lines":{"from":1,"to":96}}}},{"pageContent":"153\n\nText\n\nTag\n\n\"A great game\"\n\nSports\n\n\"The election was over\"\n\nNot sports\n\n\"Very clean match\"\n\nSports\n\n\"A clean but forgettable game\"\n\nSports\n\n\"It was a close election\"\n\nNot sports\n\nhttps://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/\n \n\nWord\n\n# in Sports\n\n# in Not sports\n\nP(Word|Sports)\n\nP(Word|Not sports)\n\n“a”\n\n2\n\n1\n\n2/11\n\n1/9\n\n“very”\n\n1\n\n0\n\n1/11\n\n0/9\n\n“close”\n\n0\n\n1\n\n0/11\n\n1/9\n\n“game”\n\n2\n\n0\n\n2/11\n\n0/9\n\nWe’ll just get a 0 probability since “close” \nnever \nappeared in the Sports=1 training data!\n\nThat’s not fair, so we have to correct for it \nsomehow...","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":153,"lines":{"from":1,"to":85}}}},{"pageContent":"154\n\nhttps://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/\n \n\nWord\n\n# in Sports\n\n# in Not sports\n\nP(Word|Sports)\n\nP(Word|Not sports)\n\n“a”\n\n2\n\n1\n\n2/11\n\n1/9\n\n“very”\n\n1\n\n0\n\n1/11\n\n0/9\n\n“close”\n\n0\n\n1\n\n0/11\n\n1/9\n\n“game”\n\n2\n\n0\n\n2/11\n\n0/9\n\nLaplace Smoothing:\n add 1 to every \ncount so nothing is ever zero!","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":154,"lines":{"from":1,"to":58}}}},{"pageContent":"155\n\nText\n\nTag\n\n\"A great game\"\n\nSports\n\n\"The election was over\"\n\nNot sports\n\n\"Very clean match\"\n\nSports\n\n\"A clean but forgettable game\"\n\nSports\n\n\"It was a close election\"\n\nNot sports\n\nhttps://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/\n \n\nWord\n\n# in Sports\n\n# in Not sports\n\nP(Word|Sports)\n\nP(Word|Not sports)\n\n“a”\n\n2\n\n1\n\n3/25\n\n2/23\n\n“very”\n\n1\n\n0\n\n2/25\n\n1/23\n\n“close”\n\n0\n\n1\n\n1/25\n\n2/23\n\n“game”\n\n2\n\n0\n\n3/25\n\n1/23\n\nWith \nLaplace Smoothing\n:\n\n\nEvery probability \nnumerator\n \nincreases by \n1","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":155,"lines":{"from":1,"to":89}}}},{"pageContent":"156\n\nText\n\nTag\n\n\"A great game\"\n\nSports\n\n\"The election was over\"\n\nNot sports\n\n\"Very clean match\"\n\nSports\n\n\"A clean but forgettable game\"\n\nSports\n\n\"It was a close election\"\n\nNot sports\n\nhttps://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/\n \n\nWord\n\n# in Sports\n\n# in Not sports\n\nP(Word|Sports)\n\nP(Word|Not sports)\n\n“a”\n\n2\n\n1\n\n3/25\n\n2/23\n\n“very”\n\n1\n\n0\n\n2/25\n\n1/23\n\n“close”\n\n0\n\n1\n\n1/25\n\n2/23\n\n“game”\n\n2\n\n0\n\n3/25\n\n1/23\n\nWith \nLaplace Smoothing\n:\n\n\nEvery probability \ndenominator\n \nincreases by the \ndistinct word count \nacross all the training data","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":156,"lines":{"from":1,"to":90}}}},{"pageContent":"157\n\nText\n\nTag\n\n\"A great game\"\n\nSports\n\n\"The election was over\"\n\nNot sports\n\n\"Very clean match\"\n\nSports\n\n\"A clean but forgettable game\"\n\nSports\n\n\"It was a close election\"\n\nNot sports\n\nhttps://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/\n \n\nWord\n\n# in Sports\n\n# in Not sports\n\nP(Word|Sports)\n\nP(Word|Not sports)\n\n“a”\n\n2\n\n1\n\n3/25\n\n2/23\n\n“very”\n\n1\n\n0\n\n2/25\n\n1/23\n\n“close”\n\n0\n\n1\n\n1/25\n\n2/23\n\n“game”\n\n2\n\n0\n\n3/25\n\n1/23\n\nWith \nLaplace Smoothing\n:\n\n\nDistinct word count overall = 14\n\n# words in Sports = 11\n+14 = 25\n\n# words in Not sports = 9\n+14 = 23","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":157,"lines":{"from":1,"to":91}}}},{"pageContent":"158\n\nhttps://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/\n \n\nWord\n\nP(Word|Sports)\n\nP(Word|Not sports)\n\n“a”\n\n3/25\n\n2/23\n\n“very”\n\n2/25\n\n1/23\n\n“close”\n\n1/25\n\n2/23\n\n“game”\n\n3/25\n\n1/23\n\nP(Sports) = \n⅗\n\nP(Not sports) = \n⅖\n \n\nP(Sports=1 | “A very close game”) is \nthe multiplication of:\n\n●\nP(“A”|Sports) = 3/25\n\n●\nP(“very”|Sports) = 2/25 \n\n●\nP(“close”|Sports) = 1/25\n\n●\nP(“game”|Sports) = 3/25\n\n●\nP(Sports) = 3/5","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":158,"lines":{"from":1,"to":59}}}},{"pageContent":"159\n\nhttps://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/\n \n\nWord\n\nP(Word|Sports)\n\nP(Word|Not sports)\n\n“a”\n\n3/25\n\n2/23\n\n“very”\n\n2/25\n\n1/23\n\n“close”\n\n1/25\n\n2/23\n\n“game”\n\n3/25\n\n1/23\n\nP(Sports) = \n⅗\n\nP(Not sports) = \n⅖\n \n\nP(Sports=1 | “A very close game”) is \nthe multiplication of:\n\n●\nP(“A”|Sports) = 3/25\n\n●\nP(“very”|Sports) = 2/25 \n\n●\nP(“close”|Sports) = 1/25\n\n●\nP(“game”|Sports) = 3/25\n\n●\nP(Sports) = 3/5\n\nWe got lucky this time because \nthis is calculable in Python still... \n0.000027648\n\n\nHow else can we deal with this P?","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":159,"lines":{"from":1,"to":66}}}},{"pageContent":"160\n\nhttps://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/\n \n\nWord\n\nP(Word|Sports)\n\nP(Word|Not sports)\n\n“a”\n\n3/25\n\n2/23\n\n“very”\n\n2/25\n\n1/23\n\n“close”\n\n1/25\n\n2/23\n\n“game”\n\n3/25\n\n1/23\n\nP(Sports) = \n⅗\n\nP(Not sports) = \n⅖\n \n\nLog[P(Sports=1 | “A very close \ngame”)] is the sum of:\n\n●\nLog[P(“A”|Sports)] = log(3/25)\n\n●\nLog[P(“very”|Sports)] = log(2/25) \n\n●\nLog[P(“close”|Sports)] = log(1/25)\n\n●\nLog[P(“game”|Sports)] = log(3/25)\n\n●\nLog[P(Sports)] = log(3/5)\n\nThis gives us -10.5","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":160,"lines":{"from":1,"to":61}}}},{"pageContent":"Log prob quiz!\n\nWhat probability has log\ne\n \n-2.3\n?\n\n\nOne in ________\n\nWhat is the log\ne\n of \nOne in 100,000\n?\n\n\n________\n\n161\n\n琀\n琀","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":161,"lines":{"from":1,"to":24}}}},{"pageContent":"Log prob quiz!\n\nWhat probability has log\ne\n \n-2.3\n?\n\n\nOne in ___\n10\n____\n\nWhat is the log\ne\n of \nOne in 100,000\n?\n\n\n___\n-11.5\n_____\n\n162","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":162,"lines":{"from":1,"to":25}}}},{"pageContent":"163\n\nhttps://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/\n \n\nWord\n\nP(Word|Sports)\n\nP(Word|Not sports)\n\n“a”\n\n3/25\n\n2/23\n\n“very”\n\n2/25\n\n1/23\n\n“close”\n\n1/25\n\n2/23\n\n“game”\n\n3/25\n\n1/23\n\nP(Sports) = \n⅗\n\nP(Not sports) = \n⅖\n \n\nLog[P(Sports=1 | “A very close \ngame”)] is the sum of:\n\n●\nLog[P(“A”|Sports)] = log(3/25)\n\n●\nLog[P(“very”|Sports)] = log(2/25) \n\n●\nLog[P(“close”|Sports)] = log(1/25)\n\n●\nLog[P(“game”|Sports)] = log(3/25)\n\n●\nLog[P(Sports)] = log(3/5)\n\nThis gives us -10.5","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":163,"lines":{"from":1,"to":61}}}},{"pageContent":"Classifying text: sports or not?\n\n164\n\nhttps://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/\n, \nhttps://www.kdnuggets.com/2020/06/naive-bayes-algorithm-everything.html\n \n \n\nFor P(Sports = 1 | “A very close game”), \nmultiply:\n\n●\nP(“A”|Sports)\n\n●\nP(“very”|Sports)\n\n●\nP(“close”|Sports)\n\n●\nP(“game”|Sports)\n\n●\nP(Sports)\n\n●\nP(“A”|Not sports)\n\n●\nP(“very”|Not sports)\n\n●\nP(“close”|Not sports)\n\n●\nP(“game”|Not sports)\n\n●\nP(Not sports)\n\nFor P(Sports = 0 | “A very close game”), \nmultiply:\n\nGoal: compare which probability is bigger!","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":164,"lines":{"from":1,"to":47}}}},{"pageContent":"Classifying text: sports or not?\n\n165\n\nhttps://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/\n, \nhttps://www.kdnuggets.com/2020/06/naive-bayes-algorithm-everything.html\n \n \n\nP(Sports = 1 | “A very close game”) \n\nɑ\n 0.000027648\n\n\nLog[P(Sports = 1 | “A very close game”)]\n\n= -10.5\n\n\nP(Sports = 0 | “A very close game”) \n\nɑ\n 0.00000572\n\n\nLog[P(Sports = 1 | “A very close game”)]\n\n= -12.07\n\nGoal: compare which probability is bigger!","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":165,"lines":{"from":1,"to":32}}}},{"pageContent":"Classifying text: sports or not?\n\n166\n\nhttps://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/\n, \nhttps://www.kdnuggets.com/2020/06/naive-bayes-algorithm-everything.html\n \n \n\nP(Sports = 1 | “A very close game”) \n\nɑ\n 0.000027648\n\n\n\n= 0.000027648 / 0.000033368 ≈ 83%\n\n\nP(Sports = 0 | “A very close game”) \n\nɑ\n 0.00000572\n\n\nLog[P(Sports = 1 | “A very close game”)]\n\n= 0.00000572 / 0.000033368 ≈ 17% \n\nIf we want to compare this without ‘proportional to’...\n\nNormalize by 0.000027648+0.00000572 = 0.000033368","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":166,"lines":{"from":1,"to":33}}}},{"pageContent":"Classifying text: sports or not?\n\n167\n\nhttps://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/\n, \nhttps://www.kdnuggets.com/2020/06/naive-bayes-algorithm-everything.html\n \n \n\nP(Sports = 1 | “A very close game”) \n\nɑ\n 0.000027648\n\n\nLog[P(Sports = 1 | “A very close game”)]\n\n= -10.5\n\n\nP(Sports = 0 | “A very close game”) \n\nɑ\n 0.00000572\n\n\nLog[P(Sports = 0 | “A very close game”)]\n\n= -12.07\n\nGoal: compare which probability is bigger!","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":167,"lines":{"from":1,"to":32}}}},{"pageContent":"Classifying text: sports or not?\n\n168\n\nhttps://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/\n, \nhttps://www.kdnuggets.com/2020/06/naive-bayes-algorithm-everything.html\n \n \n\nP(Sports = 1 | “A very close game”) \n\nɑ\n 0.000027648\n\n\nLog[P(Sports = 1 | “A very close game”)]\n\n= -10.5\n\n\nP(Sports = 0 | “A very close game”) \n\nɑ\n 0.00000572\n\n\nLog[P(Sports = 0 | “A very close game”)]\n\n= -12.07\n\nGoal: compare which probability is bigger!\n\nBetter than 1 in 100k","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":168,"lines":{"from":1,"to":34}}}},{"pageContent":"Classifying text: sports or not?\n\n169\n\nhttps://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/\n, \nhttps://www.kdnuggets.com/2020/06/naive-bayes-algorithm-everything.html\n \n \n\nP(Sports = 1 | “A very close game”) \n\nɑ\n 0.000027648\n\n\nLog[P(Sports = 1 | “A very close game”)]\n\n= -10.5\n\n\nP(Sports = 0 | “A very close game”) \n\nɑ\n 0.00000572\n\n\nLog[P(Sports = 0 | “A very close game”)]\n\n= -12.07\n\nGoal: compare which probability is bigger!\n\nWorse than 1 in \n100k","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":169,"lines":{"from":1,"to":35}}}},{"pageContent":"Classifying text: sports or not?\n\n170\n\nhttps://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/\n, \nhttps://www.kdnuggets.com/2020/06/naive-bayes-algorithm-everything.html\n \n \n\nP(Sports = 1 | “A very close game”) \n\n= 0.000027648\n\n\nLog[P(Sports = 1 | “A very close game”)]\n\n= -10.5\n\n\nP(Sports = 0 | “A very close game”) \n\n= 0.00000572\n\n\nLog[P(Sports = 0 | “A very close game”)]\n\n= -12.07\n\nIt’s more likely that “A very close game” is about Sports!","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":170,"lines":{"from":1,"to":30}}}},{"pageContent":"Probability ratios\n\n171","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":171,"lines":{"from":1,"to":3}}}},{"pageContent":"Probability ratios\n\n172\n\n= -10.5 - (-12.07) = 1.57","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":172,"lines":{"from":1,"to":5}}}},{"pageContent":"Probability ratios\n\n173\n\nExponentiating gives us how many times more likely this \ntext is Sports is vs. Not sports\n\n= -10.5 - (-12.07) = 1.57","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":173,"lines":{"from":1,"to":8}}}},{"pageContent":"Probability ratios\n\n174\n\nExponentiating gives us how many times more likely this \ntext is Sports is vs. Not sports: e\n1.57\n = \n4.8 times more \nlikely\n\n= -10.5 - (-12.07) = 1.57","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":174,"lines":{"from":1,"to":12}}}},{"pageContent":"Classifying text: sports or not?\n\n175\n\n●\nThis process was called \nNaive Bayes\n\n\n○\nGet frequencies of your data units (e.g. word)\n\n○\nCalculate probabilities assuming \nindependence of data units\n\n○\nUse Bayes’ rule to determine what \nclassification has the highest probability","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":175,"lines":{"from":1,"to":19}}}},{"pageContent":"Naive Bayes\n\n●\nNaive Bayes allows us to classify our outcomes\n\n\n○\nThese can be binary (Sports / not sports) or \nmulti-category (sport A / sport B / elections)\n\n\n○\nWe calculate probabilities based on the \nfrequencies of these categories in our data for \neach independent input x (whether x\ni\n’s are \nwords or other df inputs)\n\n176","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":176,"lines":{"from":1,"to":20}}}},{"pageContent":"Gaussian\n Naive Bayes\n\n●\nNaive Bayes allows us to classify our outcomes\n\n\n○\nThese can be binary (Sports / not sports) or \nmulti-category (sport A / sport B / elections)\n\n\n○\nWe calculate probabilities based on the \nfrequencies of these categories\n in our data for \neach independent input x (whether x\ni\n’s are \nwords or other df inputs)\n\n177\n\nnormal probability density function","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":177,"lines":{"from":1,"to":24}}}},{"pageContent":"Naive Bayes... as a model?\n\n178\n\nhttps://www.datacamp.com/tutorial/naive-bayes-scikit-learn","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":178,"lines":{"from":1,"to":5}}}},{"pageContent":"Naive Bayes... as a model?\n\n179\n\nhttps://www.datacamp.com/tutorial/naive-bayes-scikit-learn\n \n\nFit a Naive Bayes \nclassifier instead \nof a regression!","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":179,"lines":{"from":1,"to":10}}}},{"pageContent":"Naive Bayes using scikit learn\n\nfrom sklearn.naive_bayes import GaussianNB\n\n\nmodel = GaussianNB()\n\n\nmodel.fit(X_train,Y_train)\n\n\nmodel.predict(X_test)\n\n180\n\nhttps://www.datacamp.com/tutorial/naive-bayes-scikit-learn","metadata":{"source":"docs/INFO_2950/INFO2950_Lec20_20231106.pdf","pdf":{"version":"1.10.100","info":{"PDFFormatVersion":"1.7","IsAcroFormPresent":false,"IsXFAPresent":false,"Creator":"PDFium","Producer":"PDFium","CreationDate":"D:20231206223216"},"metadata":null,"totalPages":180},"loc":{"pageNumber":180,"lines":{"from":1,"to":16}}}}]