{"response":{"response":"Searching Probability Cheatsheet v2.0..."},"numbsArray":["Probability Cheatsheet v2.0"],"namespaces":["Probability Cheatsheet v2.0"],"results":{"text":"Probability is a fundamental concept in mathematics and statistics that measures the likelihood of an event occurring. It is represented as a number between 0 and 1, where 0 indicates impossibility and 1 indicates certainty. Probability is used to quantify uncertainty and make predictions in various fields, including science, economics, and everyday life.\n\nThere are different types of probability, including classical probability, empirical probability, and subjective probability. Classical probability is based on equally likely outcomes, such as flipping a fair coin or rolling a fair die. Empirical probability is based on observed frequencies from past events. Subjective probability is based on personal beliefs or judgments.\n\nThe foundation of probability theory lies in the concept of a sample space, which is the set of all possible outcomes of an experiment. Events are subsets of the sample space, and their probabilities are calculated by dividing the number of favorable outcomes by the total number of possible outcomes.\n\nProbability can be calculated using different methods, such as the counting principle, permutations, combinations, and probability distributions. The counting principle is used when the outcomes are equally likely, and it involves multiplying the number of choices at each step. Permutations are used when the order of the outcomes matters, while combinations are used when the order does not matter.\n\nProbability distributions describe the probabilities of different outcomes in a random experiment. There are two types of random variables: discrete and continuous. Discrete random variables take on a countable number of values, such as the number of heads in a series of coin flips. Continuous random variables can take on any value within a range, such as the height of a person.\n\nThe probability mass function (PMF) is used to describe the probabilities of different values for a discrete random variable. It gives the probability that the random variable takes on a specific value. The PMF must satisfy two conditions: the probability of each value must be non-negative, and the sum of all probabilities must equal 1.\n\nThe cumulative distribution function (CDF) is used to describe the probability that a random variable takes on a value less than or equal to a specific value. It is the sum of the probabilities of all values less than or equal to that value.\n\nIndependence is an important concept in probability. Two events are considered independent if the occurrence of one event does not affect the probability of the other event. Conditional probability is used to calculate the probability of an event given that another event has occurred.\n\nBayes Rule is a fundamental theorem in probability theory that allows us to update our beliefs about an event based on new evidence. It relates the conditional probability of an event given another event to the conditional probability of the second event given the first event.\n\nOverall, probability is a powerful tool for understanding and predicting the likelihood of events. It provides a framework for making informed decisions and analyzing uncertain situations.","sourceDocuments":[{"text":"B\nand\nB\nc\nas partition:\nP\n(\nA\n) =\nP\n(\nA\n|\nB\n)\nP\n(\nB\n) +\nP\n(\nA\n|\nB\nc\n)\nP\n(\nB\nc\n)\nP\n(\nA\n) =\nP\n(\nA\n∩\nB\n) +\nP\n(\nA\n∩\nB\nc\n)\nBayes’ Rule\nBayes’ Rule, and with extra conditioning (just add in\nC\n!)\nP\n(\nA\n|\nB\n) =\nP\n(\nB\n|\nA\n)\nP\n(\nA\n)\nP\n(\nB\n)\nP\n(\nA\n|\nB,C\n) =\nP\n(\nB\n|\nA,C\n)\nP\n(\nA\n|\nC\n)\nP\n(\nB\n|\nC\n)\nWe can also write\nP\n(\nA\n|\nB,C\n) =\nP\n(\nA,B,C\n)\nP\n(\nB,C\n)\n=\nP\n(\nB,C\n|\nA\n)\nP\n(\nA\n)\nP\n(\nB,C\n)\nOdds Form of Bayes’ Rule\nP\n(\nA\n|\nB\n)\nP\n(\nA\nc\n|\nB\n)\n=\nP\n(\nB\n|\nA\n)\nP\n(\nB\n|\nA\nc\n)\nP\n(\nA\n)\nP\n(\nA\nc\n)\nThe\nposterior odds\nof\nA\nare the\nlikelihood ratio\ntimes the\nprior odds\n.\nRandom Variables and their Distributions\nPMF, CDF, and Independence\nProbability Mass Function (PMF)\nGives the probability that a\ndiscrete\nrandom variable takes on the value\nx\n.\np\nX\n(\nx\n) =\nP\n(\nX\n=\nx\n)\n0\n1\n2\n3\n4\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nx\npmf\nl\nl\nl\nl\nl\nThe PMF satisfies\np\nX\n(\nx\n)\n≥\n0 and\n∑\nx\np\nX\n(\nx\n) = 1","Source":"/Users/mitz/CarlGPT/CornellGPT/docs/2/book-2.pdf","Page Number":1,"Total Pages":10},{"text":"There are several equivalent\ndefinitions.  Think about simple and extreme cases to see if you\ncan find a counterexample.\n11.\nDo a painful integral.\nIf your integral looks painful, see if\nyou can write your integral in terms of a known PDF (like\nGamma or Beta), and use the fact that PDFs integrate to 1?\n12.\nBefore moving on.\nCheck some simple and extreme cases,\ncheck whether the answer seems plausible, check for biohazards.\nBiohazards\nContributions from Jessy Hwang\n1.\nDon’t misuse the naive definition of probability.\nWhen\nanswering “What is the probability that in a group of 3 people,\nno two have the same birth month?”, it is\nnot\ncorrect to treat\nthe people as indistinguishable balls being placed into 12 boxes,\nsince that assumes the list of birth months\n{\nJanuary, January,\nJanuary\n}\nis just as likely as the list\n{\nJanuary, April, June\n}\n,\neven though the latter is six times more likely.\n2.\nDon’t confuse unconditional, conditional, and joint\nprobabilities.\nIn applying\nP\n(\nA\n|\nB\n) =\nP\n(\nB\n|\nA\n)\nP\n(\nA\n)\nP\n(\nB\n)\n, it is\nnot\ncorrect to say “\nP\n(\nB","Source":"/Users/mitz/CarlGPT/CornellGPT/docs/2/book-2.pdf","Page Number":8,"Total Pages":10},{"text":"n\n1\nn\n2\n...n\nr\npossibilities for the whole experiment.\nSampling Table\n7\n6\n5\n8\n4\n2\n9\n3\n1\nThe sampling table gives the number of possible samples of size\nk\nout\nof a population of size\nn\n, under various assumptions about how the\nsample is collected.\nOrder Matters\nNot Matter\nWith Replacement\nn\nk\n(\nn\n+\nk\n−\n1\nk\n)\nWithout Replacement\nn\n!\n(\nn\n−\nk\n)!\n(\nn\nk\n)\nNaive Definition of Probability\nIf all outcomes are equally likely, the probability of an event\nA\nhappening is:\nP\nnaive\n(\nA\n) =\nnumber of outcomes favorable to\nA\nnumber of outcomes\nThinking Conditionally\nIndependence\nIndependent Events\nA\nand\nB\nare independent if knowing whether\nA\noccurred gives no information about whether\nB\noccurred.  More\nformally,\nA\nand\nB\n(which have nonzero probability) are independent if\nand only if one of the following equivalent statements holds:\nP\n(\nA\n∩\nB\n) =\nP\n(\nA\n)\nP\n(\nB\n)\nP\n(\nA\n|\nB\n) =\nP\n(\nA\n)\nP\n(\nB\n|\nA\n) =\nP\n(\nB\n)\nConditional Independence\nA\nand\nB\nare conditionally independent\ngiven\nC\nif\nP\n(\nA\n∩\nB\n|\nC\n) =\nP\n(\nA\n|\nC\n)\nP\n(\nB\n|\nC\n).  Conditional independence\ndoes not imply independence, and independence does not imply\nconditional independence.","Source":"/Users/mitz/CarlGPT/CornellGPT/docs/2/book-2.pdf","Page Number":1,"Total Pages":10},{"text":"coin”; “Let\nX\nbe the number of successes.”)  Clear notion is\nimportant for clear thinking!  Then decide what it is that you’re\nsupposed to be finding, in terms of your notation (“I want to\nfind\nP\n(\nX\n= 3\n|\nA\n)”).  Think about what type of object your\nanswer should be (a number?  A random variable?  A PMF? A\nPDF?) and what it should be in terms of.\nTry simple and extreme cases\n.  To make an abstract experiment\nmore concrete, try\ndrawing a picture\nor making up numbers\nthat could have happened.  Pattern recognition:  does the\nstructure of the problem resemble something we’ve seen before?\n2.\nCalculating probability of an event.\nUse counting\nprinciples if the naive definition of probability applies.  Is the\nprobability of the complement easier to find?  Look for\nsymmetries.  Look for something to condition on, then apply\nBayes’ Rule or the Law of Total Probability.\n3.\nFinding the distribution of a random variable.\nFirst make\nsure you need the full distribution not just the mean (see next\nitem).  Check the\nsupport","Source":"/Users/mitz/CarlGPT/CornellGPT/docs/2/book-2.pdf","Page Number":8,"Total Pages":10},{"text":"•\nLet’s say that we have only\nb\nWeedles (failure) and\nw\nPikachus\n(success) in Viridian Forest.  We encounter\nn\nPokemon in the\nforest, and\nX\nis the number of Pikachus in our encounters.\n•\nThe number of Aces in a 5 card hand.\n•\nYou have\nw\nwhite balls and\nb\nblack balls, and you draw\nn\nballs.\nYou will draw\nX\nwhite balls.\n•\nYou have\nw\nwhite balls and\nb\nblack balls, and you draw\nn\nballs\nwithout replacement.  The number of white balls in your sample\nis HGeom(\nw,b,n\n); the number of black balls is HGeom(\nb,w,n\n).\n•\nCapture-recapture\nA forest has\nN\nelk, you capture\nn\nof them,\ntag them, and release them.  Then you recapture a new sample\nof size\nm\n.  How many tagged elk are now in the new sample?\nHGeom(\nn,N\n−\nn,m\n)\nPoisson Distribution\nLet us say that\nX\nis distributed Pois(\nλ\n).  We know the following:\nStory\nThere are rare events (low probability events) that occur many\ndifferent ways (high possibilities of occurences) at an average rate of\nλ\noccurrences per unit space or time.  The number of events that occur\nin that unit of space or time is\nX\n.\nExample","Source":"/Users/mitz/CarlGPT/CornellGPT/docs/2/book-2.pdf","Page Number":6,"Total Pages":10},{"text":"Probability Cheatsheet v2.0\nCompiled by William Chen (\nhttp://wzchen.com\n) and Joe Blitzstein,\nwith contributions from Sebastian Chiu, Yuan Jiang, Yuqi Hou, and\nJessy Hwang.  Material based on Joe Blitzstein’s (\n@stat110\n) lectures\n(\nhttp://stat110.net\n) and Blitzstein/Hwang’s Introduction to\nProbability textbook (\nhttp://bit.ly/introprobability\n).  Licensed\nunder\nCC BY-NC-SA 4.0\n.  Please share comments, suggestions, and errors\nat\nhttp://github.com/wzchen/probability_cheatsheet\n.\nLast Updated September 4, 2015\nCounting\nMultiplication Rule\ncake\nwa\nffl\ne\nS\nV\nC\nS\nV\nC\nS\nV\nC\ncake\nwa\nffl\ne\ncake\nwa\nffl\ne\ncake\nwa\nffl\ne\nLet’s say we have a compound experiment (an experiment with\nmultiple components).  If the 1st component has\nn\n1\npossible outcomes,\nthe 2nd component has\nn\n2\npossible outcomes, . . . , and the\nr\nth\ncomponent has\nn\nr\npossible outcomes, then overall there are\nn\n1\nn\n2\n...n\nr\npossibilities for the whole experiment.\nSampling Table\n7\n6\n5\n8\n4\n2\n9\n3\n1\nThe sampling table gives the number of possible samples of size\nk\nout\nof a population of size\nn","Source":"/Users/mitz/CarlGPT/CornellGPT/docs/2/book-2.pdf","Page Number":1,"Total Pages":10},{"text":"Bayes’ Rule or the Law of Total Probability.\n3.\nFinding the distribution of a random variable.\nFirst make\nsure you need the full distribution not just the mean (see next\nitem).  Check the\nsupport\nof the random variable:  what values\ncan it take on?  Use this to rule out distributions that don’t fit.\nIs there a\nstory\nfor one of the named distributions that fits the\nproblem at hand?  Can you write the random variable as a\nfunction of an r.v.  with a known distribution, say\nY\n=\ng\n(\nX\n)?\n4.\nCalculating expectation.\nIf it has a named distribution,\ncheck out the table of distributions.  If it’s a function of an r.v.\nwith a named distribution, try LOTUS. If it’s a count of\nsomething, try breaking it up into indicator r.v.s.  If you can\ncondition on something natural, consider using Adam’s law.\n5.\nCalculating variance.\nConsider independence, named\ndistributions, and LOTUS. If it’s a count of something, break it\nup into a sum of indicator r.v.s.  If it’s a sum, use properties of","Source":"/Users/mitz/CarlGPT/CornellGPT/docs/2/book-2.pdf","Page Number":8,"Total Pages":10},{"text":"different ways (high possibilities of occurences) at an average rate of\nλ\noccurrences per unit space or time.  The number of events that occur\nin that unit of space or time is\nX\n.\nExample\nA certain busy intersection has an average of 2 accidents\nper month.  Since an accident is a low probability event that can\nhappen many different ways, it is reasonable to model the number of\naccidents in a month at that intersection as Pois(2).  Then the number\nof accidents that happen in two months at that intersection is\ndistributed Pois(4).\nProperties\nLet\nX\n∼\nPois(\nλ\n1\n) and\nY\n∼\nPois(\nλ\n2\n), with\nX\n⊥⊥\nY\n.\n1.\nSum\nX\n+\nY\n∼\nPois(\nλ\n1\n+\nλ\n2\n)\n2.\nConditional\nX\n|\n(\nX\n+\nY\n=\nn\n)\n∼\nBin\n(\nn,\nλ\n1\nλ\n1\n+\nλ\n2\n)\n3.\nChicken-egg\nIf there are\nZ\n∼\nPois(\nλ\n) items and we randomly\nand independently “accept” each item with probability\np\n, then\nthe number of accepted items\nZ\n1\n∼\nPois(\nλp\n), and the number of\nrejected items\nZ\n2\n∼\nPois(\nλ\n(1\n−\np\n)), and\nZ\n1\n⊥⊥\nZ\n2\n.\nMultivariate Distributions\nMultinomial Distribution\nLet us say that the vector\n~\nX\n= (\nX\n1\n,X\n2\n,X\n3\n,...,X\nk\n)\n∼\nMult\nk\n(\nn,~p\n)\nwhere\n~p\n= (\np\n1\n,p\n2\n,...,p\nk\n).\nStory\nWe have\nn","Source":"/Users/mitz/CarlGPT/CornellGPT/docs/2/book-2.pdf","Page Number":6,"Total Pages":10},{"text":"{\nJanuary, April, June\n}\n,\neven though the latter is six times more likely.\n2.\nDon’t confuse unconditional, conditional, and joint\nprobabilities.\nIn applying\nP\n(\nA\n|\nB\n) =\nP\n(\nB\n|\nA\n)\nP\n(\nA\n)\nP\n(\nB\n)\n, it is\nnot\ncorrect to say “\nP\n(\nB\n) = 1 because we know\nB\nhappened”;\nP\n(\nB\n)\nis the\nprior\nprobability of\nB\n.  Don’t confuse\nP\n(\nA\n|\nB\n) with\nP\n(\nA,B\n).\n3.\nDon’t assume independence without justification.\nIn the\nmatching problem, the probability that card 1 is a match and\ncard 2 is a match is not 1\n/n\n2\n.  Binomial and Hypergeometric\nare often confused; the trials are independent in the Binomial\nstory and dependent in the Hypergeometric story.\n4.\nDon’t forget to do sanity checks.\nProbabilities must be\nbetween 0 and 1.  Variances must be\n≥\n0.  Supports must make\nsense.  PMFs must sum to 1.  PDFs must integrate to 1.\n5.\nDon’t confuse random variables, numbers, and events.\nLet\nX\nbe an r.v.  Then\ng\n(\nX\n) is an r.v.  for any function\ng\n.  In\nparticular,\nX\n2\n,\n|\nX\n|\n,\nF\n(\nX\n), and\nI\nX>\n3\nare r.v.s.\nP\n(\nX\n2\n< X\n|\nX\n≥\n0)\n,E\n(\nX\n)\n,\nVar(\nX\n)\n,\nand\ng\n(\nE\n(\nX\n)) are numbers.\nX\n= 2 and\nF\n(\nX\n)\n≥−","Source":"/Users/mitz/CarlGPT/CornellGPT/docs/2/book-2.pdf","Page Number":8,"Total Pages":10},{"text":"Example\nLet\nX\nbe the indicator of Heads for a fair coin toss.  Then\nX\n∼\nBern(\n1\n2\n).  Also, 1\n−\nX\n∼\nBern(\n1\n2\n) is the indicator of Tails.\nBinomial Distribution\n0\n2\n4\n6\n8\n10\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\nx\npmf\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\nl\n0\n2\n4\n6\n8\n10\nBin(10,1/2)\nLet us say that\nX\nis distributed Bin(\nn,p\n).  We know the following:\nStory\nX\nis the number of “successes” that we will achieve in\nn\nindependent trials, where each trial is either a success or a failure, each\nwith the same probability\np\nof success.  We can also write\nX\nas a sum\nof multiple independent Bern(\np\n) random variables.  Let\nX\n∼\nBin(\nn,p\n)\nand\nX\nj\n∼\nBern(\np\n), where all of the Bernoullis are independent.  Then\nX\n=\nX\n1\n+\nX\n2\n+\nX\n3\n+\n···\n+\nX\nn\nExample\nIf Jeremy Lin makes 10 free throws and each one\nindependently has a\n3\n4\nchance of getting in, then the number of free\nthrows he makes is distributed Bin(10\n,\n3\n4\n).\nProperties\nLet\nX\n∼\nBin(\nn,p\n)\n,Y\n∼\nBin(\nm,p\n) with\nX\n⊥⊥\nY\n.\n•\nRedefine success\nn\n−\nX\n∼\nBin(\nn,\n1\n−\np\n)\n•\nSum\nX\n+\nY\n∼\nBin(\nn\n+\nm,p\n)\n•\nConditional\nX\n|\n(\nX\n+\nY\n=\nr\n)\n∼\nHGeom(\nn,m,r\n)\n•\nBinomial-Poisson Relationship\nBin(\nn,p\n) is approximately\nPois(\nλ\n) if\np\nis small.\n•","Source":"/Users/mitz/CarlGPT/CornellGPT/docs/2/book-2.pdf","Page Number":6,"Total Pages":10}]}}